todo:
- check for correlation between utterances and utility?
- maybe break the problem down, and test each part standalone somehow?
- (change number of utteracne tokens from 10 to 11)
- probably should rerun with comms turned off
- should we do also non-prosocial?

done:
- add reward
- add backprop
- add logging + draw graph
- load/save model
- invalid proposals should cause reward 0 ...
- if no termination, then reward 0
- add baselining control variate
- fix gru => lstm
- remove termination token
- check N for forward batching
- add forward batching
- add reward for batching
- add backward batching
- add utterance back into net
- print example utterances/conversations
- add in game length to logs somehow
- add entropy regularization
- (cudaify)
- enable cuda for utterances
- add entropy regularization for utterances
- add logging of proportion of stochastic draws matching argmax
- add option to remove stochasticity, pick argmax, for testing
- add train/test split
- add test to training loop
